{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Assignment : NLP Introduction & Text Processing | Assignment**"
      ],
      "metadata": {
        "id": "Jd3OsYX_KXad"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08f3fb16"
      },
      "source": [
        "**Question 1: What is Computational Linguistics and how does it relate to NLP?**\n",
        "\n",
        "Ans: Computational Linguistics (CL) is an interdisciplinary field that deals with the statistical or rule-based modeling of natural language from a computational perspective. It is concerned with the theoretical aspects of using computers to understand and process human language.\n",
        "\n",
        "Natural Language Processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
        "\n",
        "Essentially, CL provides the linguistic theories and models, while NLP focuses on building practical systems and applications that can understand and process human language using those theories and models. You can think of CL as the science and NLP as the engineering that stems from that science."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "916453fd"
      },
      "source": [
        "**Question 2: Briefly describe the historical evolution of Natural Language Processing.**\n",
        "\n",
        "Ans : The history of NLP can be broadly divided into several periods:\n",
        "\n",
        "*   **Early Years (1950s-1960s):** Focused on rule-based approaches, with attempts to translate languages using grammatical rules and dictionaries. This era saw the development of early systems like Georgetown-IBM experiment.\n",
        "*   **Statistical Revolution (1970s-1980s):** Shifted towards statistical methods, using probabilities and machine learning techniques to process language. This was driven by the availability of larger datasets and increased computational power.\n",
        "*   **Machine Learning Era (1990s-2000s):** Saw significant advancements in machine learning techniques applied to NLP tasks, such as support vector machines, decision trees, and hidden Markov models. This led to improved performance in areas like part-of-speech tagging and named entity recognition.\n",
        "*   **Deep Learning Era (2010s-Present):** Characterized by the rise of deep learning models, particularly neural networks. Techniques like recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformers have revolutionized NLP, achieving state-of-the-art results in various tasks, including machine translation, text generation, and sentiment analysis. The availability of massive datasets and powerful hardware like GPUs has been crucial for this progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61a8d17c"
      },
      "source": [
        "**Question 3: List and explain three major use cases of NLP in today’s tech industry.**\n",
        "\n",
        "Three major use cases of NLP in today's tech industry are:\n",
        "\n",
        "1.  **Chatbots and Virtual Assistants:** NLP enables the development of chatbots and virtual assistants (like Siri, Alexa, and Google Assistant) that can understand and respond to human language. They are used in customer service, information retrieval, and task automation.\n",
        "2.  **Sentiment Analysis:** NLP is used to analyze text data (social media posts, reviews, feedback) to determine the sentiment expressed (positive, negative, neutral). This is valuable for businesses to understand customer opinions, brand perception, and market trends.\n",
        "3.  **Machine Translation:** NLP powers machine translation services (like Google Translate) that automatically translate text or speech from one language to another. This facilitates communication and access to information across language barriers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81f01ad8"
      },
      "source": [
        "**Question 4: What is text normalization and why is it essential in text processing tasks?**\n",
        "\n",
        "Ans: Text normalization is the process of transforming text into a canonical (standard) form. This involves tasks such as converting text to lowercase, removing punctuation, correcting spelling errors, and handling numbers and dates in a consistent way.\n",
        "\n",
        "It is essential in text processing tasks because it reduces variations in the text data, making it easier for algorithms to process and analyze. For example, without normalization, words like \"run,\" \"running,\" and \"ran\" might be treated as distinct tokens, even though they refer to the same concept. Normalization helps to group these variations together, improving the accuracy and efficiency of NLP models."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8UgFvVSMfGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe7cc392"
      },
      "source": [
        "**Question 5: Compare and contrast stemming and lemmatization with suitable examples.**\n",
        "\n",
        "Ans : Both stemming and lemmatization are techniques used in text processing to reduce words to their base or root form. However, they differ in their approach and the quality of the resulting root form:\n",
        "\n",
        "**Stemming:**\n",
        "\n",
        "*   **Approach:** Stemming is a simpler, rule-based process that chops off the ends of words to arrive at a root form. It's often a heuristic process that doesn't consider the word's context or meaning.\n",
        "*   **Result:** The resulting stem may not be a valid word.\n",
        "*   **Speed:** Generally faster than lemmatization.\n",
        "*   **Use cases:** Often used in information retrieval systems where speed is crucial and perfect accuracy of the root form is not strictly necessary.\n",
        "\n",
        "**Example of Stemming:**\n",
        "\n",
        "*   \"running\" -> \"run\"\n",
        "*   \"flies\" -> \"fli\"\n",
        "*   \"studies\" -> \"studi\"\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        "*   **Approach:** Lemmatization is a more sophisticated process that uses vocabulary and morphological analysis to return the base or dictionary form of a word, known as the lemma. It considers the word's context and meaning.\n",
        "*   **Result:** The resulting lemma is always a valid word.\n",
        "*   **Speed:** Generally slower than stemming as it requires more linguistic knowledge.\n",
        "*   **Use cases:** Preferred in applications where the meaning and grammatical correctness of the root form are important, such as in natural language understanding and machine translation.\n",
        "\n",
        "**Example of Lemmatization:**\n",
        "\n",
        "*   \"running\" -> \"run\"\n",
        "*   \"flies\" -> \"fly\"\n",
        "*   \"studies\" -> \"study\"\n",
        "*   \"better\" -> \"good\" (lemmatization can handle irregular forms)\n",
        "\n",
        "**Key Differences Summarized:**\n",
        "\n",
        "| Feature        | Stemming                       | Lemmatization                     |\n",
        "| :------------- | :----------------------------- | :-------------------------------- |\n",
        "| **Approach**   | Rule-based, heuristic          | Dictionary and morphological analysis |\n",
        "| **Result**     | May not be a valid word        | Always a valid word               |\n",
        "| **Speed**      | Faster                         | Slower                            |\n",
        "| **Context**    | Does not consider context      | Considers context                 |\n",
        "| **Complexity** | Simpler                        | More complex                      |\n",
        "\n",
        "In essence, lemmatization is more accurate and linguistically informed than stemming, but it is also computationally more expensive. The choice between stemming and lemmatization depends on the specific requirements of the NLP task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 6: Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:\n",
        "\n",
        " “Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”"
      ],
      "metadata": {
        "id": "NioICy2_NJAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\n",
        "\n",
        "# Regex pattern for email addresses\n",
        "# This pattern is a simplified one for demonstration.\n",
        "# A more robust pattern would be needed for real-world scenarios.\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Find all email addresses in the text\n",
        "email_addresses = re.findall(email_pattern, text)\n",
        "\n",
        "# Print the extracted email addresses\n",
        "print(\"Extracted email addresses:\")\n",
        "for email in email_addresses:\n",
        "    print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N6XyYfUMvwA",
        "outputId": "a9381e6c-81f0-4e70-896f-fac361e3a03d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted email addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1JaIweycNizR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eec4ecaa"
      },
      "source": [
        "**Question 7: Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:**\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.”"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qu7VaFE5O4UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95050f49",
        "outputId": "7a11a2d1-b55a-4a19-ae7c-ef84b465a152"
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a37ca9f",
        "outputId": "61b8cfdf-1a7a-4017-b91e-530e8667cdcb"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Calculate frequency distribution\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "# Print the tokens and frequency distribution\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(fdist.most_common(10)) # Display the 10 most common tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "[(',', 7), ('.', 4), ('NLP', 3), ('and', 3), ('is', 2), ('of', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "268bd3ba"
      },
      "source": [
        "**Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06aa8815",
        "outputId": "d8eb9768-8283-4b8f-c6eb-46421a34b153"
      },
      "source": [
        "# Install spaCy\n",
        "!pip install spacy\n",
        "\n",
        "# Download a spaCy language model (e.g., English)\n",
        "# You might need to restart the runtime after this step\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9780533",
        "outputId": "f97b10e0-9ee7-4abb-b970-739e7435b7c5"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading en_core_web_sm model...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Barack Obama was the 44th President of the United States of America. He was born in Honolulu, Hawaii.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Identify and label proper nouns\n",
        "print(\"Proper Nouns:\")\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        print(f\"{token.text} ({token.pos_})\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns:\n",
            "Barack (PROPN)\n",
            "Obama (PROPN)\n",
            "President (PROPN)\n",
            "United (PROPN)\n",
            "States (PROPN)\n",
            "America (PROPN)\n",
            "Honolulu (PROPN)\n",
            "Hawaii (PROPN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3bzSXqxQ8MY"
      },
      "source": [
        "**Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17244df3",
        "outputId": "acff7484-e920-408b-f865-bcbe55235c4f"
      },
      "source": [
        "# Install Gensim\n",
        "!pip install gensim"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d89c8d63",
        "outputId": "3bfa1b2f-0396-479a-ac5b-8e9f6d337dd6"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_https_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Preprocess the dataset (tokenize and lowercase)\n",
        "processed_sentences = [word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(processed_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Print the vocabulary size\n",
        "print(f\"Vocabulary size: {len(model.wv)}\")\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word_vector = model.wv['language']\n",
        "print(f\"\\nVector for 'language': {word_vector[:10]}...\") # Print first 10 elements\n",
        "\n",
        "# Example: Find similar words\n",
        "similar_words = model.wv.most_similar('language')\n",
        "print(f\"\\nWords similar to 'language': {similar_words}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 46\n",
            "\n",
            "Vector for 'language': [-0.00958061  0.00894419  0.00416531  0.00923353  0.00664613  0.00292132\n",
            "  0.00980621 -0.004423   -0.0067969   0.00421717]...\n",
            "\n",
            "Words similar to 'language': [('technique', 0.28540539741516113), ('text', 0.19906380772590637), ('processing', 0.19070622324943542), ('are', 0.10012832283973694), ('step', 0.09662030637264252), ('is', 0.07467351853847504), ('that', 0.07278265804052353), ('representation', 0.060818735510110855), ('meaning', 0.04675615578889847), ('modeling', 0.044769588857889175)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FX1paQ48RCG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13e1583"
      },
      "source": [
        "**Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6embX4XROwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdf5e6e"
      },
      "source": [
        "Here is an outline of the steps to clean, process, and extract useful insights from thousands of customer reviews using NLP techniques:\n",
        "\n",
        "1.  **Data Loading and Inspection:**\n",
        "    *   Load the customer feedback data from its source (e.g., CSV file, database, API).\n",
        "    *   Inspect the data to understand its structure, identify relevant columns (e.g., review text, rating, timestamp), and check for missing values or inconsistencies.\n",
        "\n",
        "2.  **Text Cleaning and Preprocessing:**\n",
        "    *   **Handle missing values:** Decide how to deal with missing reviews (e.g., remove rows, impute with a placeholder).\n",
        "    *   **Convert to lowercase:** Standardize the text by converting everything to lowercase.\n",
        "    *   **Remove punctuation and special characters:** Eliminate characters that do not contribute to the meaning of the text.\n",
        "    *   **Remove numbers:** Decide whether to remove or replace numbers depending on the analysis goals.\n",
        "    *   **Remove stopwords:** Eliminate common words (e.g., \"the,\" \"a,\" \"is\") that have little analytical value.\n",
        "    *   **Perform stemming or lemmatization:** Reduce words to their root form to group similar words together (choose based on the task's needs).\n",
        "    *   **Handle emojis and emoticons:** Decide how to process or remove these based on whether they convey sentiment.\n",
        "    *   **Correct spelling errors:** Use spell-checking techniques if necessary.\n",
        "\n",
        "3.  **Text Representation (Feature Extraction):**\n",
        "    *   Convert the cleaned text into numerical representations that can be used by machine learning models. Common techniques include:\n",
        "        *   **Bag-of-Words (BoW):** Represents text as a bag of its words, ignoring grammar and word order but keeping multiplicity.\n",
        "        *   **TF-IDF (Term Frequency-Inverse Document Frequency):** Weights words based on their frequency in a document and their inverse frequency across the entire dataset, highlighting important words.\n",
        "        *   **Word Embeddings (e.g., Word2Vec, GloVe, FastText):** Represents words as dense vectors in a continuous vector space, capturing semantic relationships between words.\n",
        "        *   **Sentence Embeddings (e.g., Sentence-BERT):** Represents entire sentences as vectors, capturing the meaning of the sentence.\n",
        "\n",
        "4.  **Exploratory Data Analysis (EDA) with NLP:**\n",
        "    *   **Word clouds:** Visualize the most frequent words in the reviews.\n",
        "    *   **N-gram analysis:** Analyze the frequency of sequences of words (bigrams, trigrams) to identify common phrases.\n",
        "    *   **Sentiment analysis:** Apply sentiment analysis techniques (lexicon-based or machine learning-based) to determine the overall sentiment of reviews.\n",
        "    *   **Topic modeling (e.g., LDA, NMF):** Discover underlying topics or themes present in the reviews.\n",
        "\n",
        "5.  **Insight Extraction and Modeling:**\n",
        "    *   **Sentiment analysis:** Categorize reviews as positive, negative, or neutral to understand customer satisfaction.\n",
        "    *   **Aspect-based sentiment analysis:** Identify the specific aspects of the product or service that customers are talking about and their sentiment towards those aspects.\n",
        "    *   **Keyphrase extraction:** Identify important phrases or keywords that summarize the main points of the reviews.\n",
        "    *   **Text classification:** Train models to classify reviews into predefined categories (e.g., bug report, feature request, usability issue).\n",
        "    *   **Relationship extraction:** Identify relationships between entities in the text (e.g., \"app\" is \"slow\").\n",
        "\n",
        "6.  **Visualization and Reporting:**\n",
        "    *   Visualize the insights gained from the analysis (e.g., sentiment distribution, topic trends, keyphrase frequency).\n",
        "    *   Create reports or dashboards to communicate the findings to stakeholders, highlighting actionable insights for product improvement, customer service, or marketing.\n",
        "\n",
        "7.  **Model Evaluation (if machine learning models are used):**\n",
        "    *   Evaluate the performance of any trained models using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n",
        "\n",
        "8.  **Monitoring and Iteration:**\n",
        "    *   Continuously monitor new incoming reviews and periodically re-run the analysis to track changes in customer feedback and identify emerging issues or trends. Refine the NLP pipeline and models as needed."
      ]
    }
  ]
}